{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkN8reOe4q9N"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cd drive"
      ],
      "metadata": {
        "id": "hjTPM0bU8zIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "8DV-dziQFz1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfGdl1OH866E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd MyDrive"
      ],
      "metadata": {
        "id": "ftaa3GBIHJ09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  !ls"
      ],
      "metadata": {
        "id": "zc0teHQzHHSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMC4Js4jWwyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LdH3s20Ajdsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive\n"
      ],
      "metadata": {
        "id": "DNfbJjGVjrc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "3op7mCFGjtci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd MyDrive"
      ],
      "metadata": {
        "id": "D5N5-9Ddjuf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "N6lJ9ssdjwlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from config_entity import DataIngestionConfig"
      ],
      "metadata": {
        "id": "EMNXzbNPjyBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-box==6.0.2"
      ],
      "metadata": {
        "id": "6mrw71OJkKLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install types-PyYAML\n"
      ],
      "metadata": {
        "id": "-DspB5zwlCDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyYAML"
      ],
      "metadata": {
        "id": "xYuiXUkEnX9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ensure==1.0.2"
      ],
      "metadata": {
        "id": "Khr7jROHnZj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib"
      ],
      "metadata": {
        "id": "Pz4tqhqingBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1VnfW3bZnkZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "ycdBY-5qnnup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "from box import Box \n",
        "from box.exceptions import BoxValueError\n",
        "\n",
        "import json\n",
        "import joblib\n",
        "from ensure import ensure_annotations\n",
        "from box import ConfigBox\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "@ensure_annotations\n",
        "def read_yaml(path_to_yaml: Path) -> ConfigBox:\n",
        "    \"\"\"reads yaml file and returns\n",
        "    Args:\n",
        "        path_to_yaml (str): path like input\n",
        "    Raises:\n",
        "        ValueError: if yaml file is empty\n",
        "        e: empty file\n",
        "    Returns:\n",
        "        ConfigBox: ConfigBox type\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path_to_yaml) as yaml_file:\n",
        "            content = yaml.safe_load(yaml_file)\n",
        "            return ConfigBox(content)\n",
        "    except BoxValueError:\n",
        "        raise ValueError(\"yaml file is empty\")\n",
        "    except Exception as e:\n",
        "        raise \n",
        "@ensure_annotations\n",
        "def create_directories(path_to_directories: list, verbose=True):\n",
        "    \"\"\"create list of directories\n",
        "    Args:\n",
        "        path_to_directories (list): list of path of directories\n",
        "        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\n",
        "    \"\"\"\n",
        "    for path in path_to_directories:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "       \n",
        "\n",
        "@ensure_annotations\n",
        "def save_json(path: Path, data: dict):\n",
        "    \"\"\"save json data\n",
        "    Args:\n",
        "        path (Path): path to json file\n",
        "        data (dict): data to be saved in json file\n",
        "    \"\"\"\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "\n",
        "@ensure_annotations\n",
        "def load_json(path: Path) -> ConfigBox:\n",
        "    \"\"\"load json files data\n",
        "    Args:\n",
        "        path (Path): path to json file\n",
        "    Returns:\n",
        "        ConfigBox: data as class attributes instead of dict\n",
        "    \"\"\"\n",
        "    with open(path) as f:\n",
        "        content = json.load(f)\n",
        "\n",
        "    return ConfigBox(content)\n",
        "\n",
        "@ensure_annotations\n",
        "def save_bin(data: Any, path: Path):\n",
        "    \"\"\"save binary file\n",
        "    Args:\n",
        "        data (Any): data to be saved as binary\n",
        "        path (Path): path to binary file\n",
        "    \"\"\"\n",
        "    joblib.dump(value=data, filename=path)\n",
        "\n",
        "@ensure_annotations\n",
        "def load_bin(path: Path) -> Any:\n",
        "    \"\"\"load binary data\n",
        "    Args:\n",
        "        path (Path): path to binary file\n",
        "    Returns:\n",
        "        Any: object stored in the file\n",
        "    \"\"\"\n",
        "    data = joblib.load(path)\n",
        "\n",
        "    return data\n",
        "\n",
        "@ensure_annotations\n",
        "def get_size(path: Path) -> str:\n",
        "    \"\"\"get size in KB\n",
        "    Args:\n",
        "        path (Path): path of the file\n",
        "    Returns:\n",
        "        str: size in KB\n",
        "    \"\"\"\n",
        "    size_in_kb = round(os.path.getsize(path)/1024)\n",
        "    return f\"~ {size_in_kb} KB\""
      ],
      "metadata": {
        "id": "99KU_zJInsOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DataIngestionConfig:\n",
        "    root_dir: Path\n",
        "    source_URL: str\n",
        "    local_data_file: Path\n",
        "    unzip_dir: Path"
      ],
      "metadata": {
        "id": "W69N1mVjoDdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "CONFIG_FILE_PATH = Path(\"config.yaml\")\n",
        "PARAMS_FILE_PATH = Path(\"params.yaml\")"
      ],
      "metadata": {
        "id": "Uow_4vnGphWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_FILE_PATH"
      ],
      "metadata": {
        "id": "o07bfN34qQ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the configuration to read the yaml file and create the directory\n",
        "class ConfigurationManager:\n",
        "    def __init__(\n",
        "        self, \n",
        "        config_filepath = CONFIG_FILE_PATH,\n",
        "        params_filepath = PARAMS_FILE_PATH):\n",
        "        self.config = read_yaml(config_filepath)\n",
        "        self.params = read_yaml(params_filepath)\n",
        "        create_directories([self.config.artifacts_root])\n",
        "\n",
        "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
        "        config = self.config.data_ingestion\n",
        "        \n",
        "        create_directories([config.root_dir])\n",
        "\n",
        "        data_ingestion_config = DataIngestionConfig(\n",
        "            root_dir=config.root_dir,\n",
        "            source_URL=config.source_URL,\n",
        "            local_data_file=config.local_data_file,\n",
        "            unzip_dir=config.unzip_dir \n",
        "        )\n",
        "\n",
        "        return data_ingestion_config"
      ],
      "metadata": {
        "id": "-Sfkq80KpQJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request as request\n",
        "from zipfile import ZipFile\n",
        "\n",
        "class DataIngestion:\n",
        "    def __init__(self, config: DataIngestionConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def download_file(self):\n",
        "        if not os.path.exists(self.config.local_data_file):\n",
        "            filename, headers = request.urlretrieve(\n",
        "                url = self.config.source_URL,\n",
        "                filename = self.config.local_data_file\n",
        "            )\n",
        "\n",
        "    def _get_updated_list_of_files(self, list_of_files):\n",
        "        return [f for f in list_of_files if f.endswith(\".jpg\") and (\"Cat\" in f or \"Dog\" in f)] #return only cat or dog files\n",
        "\n",
        "    def _preprocess(self, zf: ZipFile, f: str, working_dir: str):\n",
        "        target_filepath = os.path.join(working_dir, f)\n",
        "        if not os.path.exists(target_filepath):\n",
        "            zf.extract(f, working_dir)\n",
        "        \n",
        "        if os.path.getsize(target_filepath) == 0:\n",
        "            os.remove(target_filepath) \n",
        "\n",
        "    def unzip_and_clean(self):\n",
        "        with ZipFile(file=self.config.local_data_file, mode=\"r\") as zf:\n",
        "            list_of_files = zf.namelist()\n",
        "            updated_list_of_files = self._get_updated_list_of_files(list_of_files) #hidden method to check files present or not /x/y/Cat/1.jpg\n",
        "            #preprocessing ,check either they are present or not and have size or not\n",
        "            for f in updated_list_of_files:   \n",
        "                self._preprocess(zf, f, self.config.unzip_dir)"
      ],
      "metadata": {
        "id": "7MZkPd5EqV0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    config = ConfigurationManager()\n",
        "    data_ingestion_config = config.get_data_ingestion_config()\n",
        "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
        "    data_ingestion.download_file()\n",
        "    data_ingestion.unzip_and_clean()\n",
        "except Exception as e:\n",
        "    raise e"
      ],
      "metadata": {
        "id": "JzimUalSqeP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = os.path.isfile(CONFIG_FILE_PATH)\n",
        "print(file)"
      ],
      "metadata": {
        "id": "345djtiMqlXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass(frozen=True)\n",
        "class PrepareBaseModelConfig:\n",
        "    root_dir: Path\n",
        "    base_model_path: Path\n",
        "    updated_base_model_path: Path\n",
        "    params_image_size: list\n",
        "    params_learning_rate: float\n",
        "    params_include_top: bool\n",
        "    params_weights: str\n",
        "    params_classes: int"
      ],
      "metadata": {
        "id": "vofFizGnrPm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigurationManager:\n",
        "    def __init__(\n",
        "        self, \n",
        "        config_filepath = CONFIG_FILE_PATH,\n",
        "        params_filepath = PARAMS_FILE_PATH):\n",
        "        self.config = read_yaml(config_filepath)\n",
        "        self.params = read_yaml(params_filepath)\n",
        "        create_directories([self.config.artifacts_root])\n",
        "\n",
        "    def  get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
        "        config = self.config.prepare_base_model\n",
        "        \n",
        "        create_directories([config.root_dir])\n",
        "\n",
        "        prepare_base_model_config = PrepareBaseModelConfig(\n",
        "            root_dir=Path(config.root_dir),\n",
        "            base_model_path=Path(config.base_model_path),\n",
        "            updated_base_model_path=Path(config.updated_base_model_path),\n",
        "            params_image_size=self.params.IMAGE_SIZE,\n",
        "            params_learning_rate=self.params.LEARNING_RATE,\n",
        "            params_include_top=self.params.INCLUDE_TOP,\n",
        "            params_weights=self.params.WEIGHTS,\n",
        "            params_classes=self.params.CLASSES\n",
        "        )\n",
        "\n",
        "        return prepare_base_model_config"
      ],
      "metadata": {
        "id": "NN_qytOzs8ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request as request\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf\n",
        "\n",
        "class PrepareBaseModel:\n",
        "    def __init__(self, config: PrepareBaseModelConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def get_base_model(self):\n",
        "        self.model = tf.keras.applications.vgg16.VGG16(\n",
        "            input_shape=self.config.params_image_size,\n",
        "            weights=self.config.params_weights,\n",
        "            include_top=self.config.params_include_top\n",
        "        )\n",
        "\n",
        "        self.save_model(path=self.config.base_model_path, model=self.model)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_full_model(model, classes, freeze_all, freeze_till, learning_rate):\n",
        "        if freeze_all:\n",
        "            for layer in model.layers:\n",
        "                model.trainable = False\n",
        "        elif (freeze_till is not None) and (freeze_till > 0):\n",
        "            for layer in model.layers[:-freeze_till]:\n",
        "                model.trainable = False\n",
        "\n",
        "        flatten_in = tf.keras.layers.Flatten()(model.output)\n",
        "        prediction = tf.keras.layers.Dense(\n",
        "            units=classes,\n",
        "            activation=\"softmax\"\n",
        "        )(flatten_in)\n",
        "\n",
        "        full_model = tf.keras.models.Model(\n",
        "            inputs=model.input,\n",
        "            outputs=prediction\n",
        "        )\n",
        "\n",
        "        full_model.compile(\n",
        "            optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
        "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "\n",
        "        full_model.summary()\n",
        "        return full_model\n",
        "\n",
        "    def update_base_model(self):\n",
        "        self.full_model = self._prepare_full_model(\n",
        "            model=self.model,\n",
        "            classes=self.config.params_classes,\n",
        "            freeze_all=True,\n",
        "            freeze_till=None,\n",
        "            learning_rate=self.config.params_learning_rate\n",
        "        )\n",
        "\n",
        "        self.save_model(path=self.config.updated_base_model_path, model=self.full_model)\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(path: Path, model: tf.keras.Model):\n",
        "        model.save(path)"
      ],
      "metadata": {
        "id": "b_XLf1xMtCdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "try:\n",
        "    config = ConfigurationManager()\n",
        "    prepare_base_model_config = config.get_prepare_base_model_config()\n",
        "    prepare_base_model = PrepareBaseModel(config=prepare_base_model_config)\n",
        "    prepare_base_model.get_base_model()\n",
        "    prepare_base_model.update_base_model()\n",
        "except Exception as e:\n",
        "    raise e"
      ],
      "metadata": {
        "id": "GkaD8W4BtI1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class PrepareCallbacksConfig:\n",
        "    root_dir: Path\n",
        "    tensorboard_root_log_dir: Path\n",
        "    checkpoint_model_filepath: Path"
      ],
      "metadata": {
        "id": "r7OWjh0OtPRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigurationManager:\n",
        "    def __init__(\n",
        "        self, \n",
        "        config_filepath = CONFIG_FILE_PATH,\n",
        "        params_filepath = PARAMS_FILE_PATH):\n",
        "        self.config = read_yaml(config_filepath)\n",
        "        self.params = read_yaml(params_filepath)\n",
        "        create_directories([self.config.artifacts_root])\n",
        "\n",
        "    def get_prepare_callback_config(self) -> PrepareCallbacksConfig:\n",
        "        config = self.config.prepare_callbacks\n",
        "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
        "        create_directories([\n",
        "            Path(model_ckpt_dir),\n",
        "            Path(config.tensorboard_root_log_dir)\n",
        "        ])\n",
        "\n",
        "        prepare_callback_config = PrepareCallbacksConfig(\n",
        "            root_dir=Path(config.root_dir),\n",
        "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir),\n",
        "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath)\n",
        "        )\n",
        "\n",
        "        return prepare_callback_config"
      ],
      "metadata": {
        "id": "D2Z-x26ptakv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request as request\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "class PrepareCallback:\n",
        "    def __init__(self, config: PrepareCallbacksConfig):\n",
        "        self.config = config\n",
        "\n",
        "    @property\n",
        "    def _create_tb_callbacks(self):\n",
        "        timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "        tb_running_log_dir = os.path.join(\n",
        "            self.config.tensorboard_root_log_dir,\n",
        "            f\"tb_logs_at_{timestamp}\",\n",
        "        )\n",
        "        return tf.keras.callbacks.TensorBoard(log_dir=tb_running_log_dir)\n",
        "\n",
        "    @property\n",
        "    def _create_ckpt_callbacks(self):\n",
        "        return tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=self.config.checkpoint_model_filepath,\n",
        "            save_best_only=True\n",
        "        )\n",
        "\n",
        "    def get_tb_ckpt_callbacks(self):\n",
        "        return [\n",
        "            self._create_tb_callbacks,\n",
        "            self._create_ckpt_callbacks\n",
        "        ]"
      ],
      "metadata": {
        "id": "0H1c5aG3td2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    config = ConfigurationManager()\n",
        "    prepare_callbacks_config = config.get_prepare_callback_config()\n",
        "    prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
        "    callback_list = prepare_callbacks.get_tb_ckpt_callbacks()\n",
        "    \n",
        "except Exception as e:\n",
        "    raise e"
      ],
      "metadata": {
        "id": "lHTpnYKlthgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "    root_dir: Path\n",
        "    trained_model_path: Path\n",
        "    updated_base_model_path: Path\n",
        "    training_data: Path\n",
        "    params_epochs: int\n",
        "    params_batch_size: int\n",
        "    params_is_augmentation: bool\n",
        "    params_image_size: list"
      ],
      "metadata": {
        "id": "8UIEQ1ZgtlCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigurationManager:\n",
        "    def __init__(\n",
        "        self, \n",
        "        config_filepath = CONFIG_FILE_PATH,\n",
        "        params_filepath = PARAMS_FILE_PATH):\n",
        "        self.config = read_yaml(config_filepath)\n",
        "        self.params = read_yaml(params_filepath)\n",
        "        create_directories([self.config.artifacts_root])\n",
        "\n",
        "    def get_prepare_callback_config(self) -> PrepareCallbacksConfig:\n",
        "        config = self.config.prepare_callbacks\n",
        "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
        "        create_directories([\n",
        "            Path(model_ckpt_dir),\n",
        "            Path(config.tensorboard_root_log_dir)\n",
        "        ])\n",
        "\n",
        "        prepare_callback_config = PrepareCallbacksConfig(\n",
        "            root_dir=Path(config.root_dir),\n",
        "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir),\n",
        "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath)\n",
        "        )\n",
        "\n",
        "        return prepare_callback_config\n",
        "\n",
        "    def get_training_config(self) -> TrainingConfig:\n",
        "        training = self.config.training\n",
        "        prepare_base_model = self.config.prepare_base_model\n",
        "        params = self.params\n",
        "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"PetImages\")\n",
        "        create_directories([\n",
        "            Path(training.root_dir)\n",
        "        ])\n",
        "\n",
        "        training_config = TrainingConfig(\n",
        "            root_dir=Path(training.root_dir),\n",
        "            trained_model_path=Path(training.trained_model_path),\n",
        "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
        "            training_data=Path(training_data),\n",
        "            params_epochs=params.EPOCHS,\n",
        "            params_batch_size=params.BATCH_SIZE,\n",
        "            params_is_augmentation=params.AUGMENTATION,\n",
        "            params_image_size=params.IMAGE_SIZE\n",
        "        )\n",
        "\n",
        "        return training_config"
      ],
      "metadata": {
        "id": "WjuScEWCtsVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class PrepareCallback:\n",
        "    def __init__(self, config: PrepareCallbacksConfig):\n",
        "        self.config = config\n",
        "\n",
        "    @property\n",
        "    def _create_tb_callbacks(self):\n",
        "        timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "        tb_running_log_dir = os.path.join(\n",
        "            self.config.tensorboard_root_log_dir,\n",
        "            f\"tb_logs_at_{timestamp}\",\n",
        "        )\n",
        "        return tf.keras.callbacks.TensorBoard(log_dir=tb_running_log_dir)\n",
        "\n",
        "    @property\n",
        "    def _create_ckpt_callbacks(self):\n",
        "        return tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=self.config.checkpoint_model_filepath,\n",
        "            save_best_only=True\n",
        "        )\n",
        "\n",
        "    def get_tb_ckpt_callbacks(self):\n",
        "        return [\n",
        "            self._create_tb_callbacks,\n",
        "            self._create_ckpt_callbacks\n",
        "        ]"
      ],
      "metadata": {
        "id": "L5cf0ALztx75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request as request\n",
        "from zipfile import ZipFile\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "class Training:\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def get_base_model(self):\n",
        "        self.model = tf.keras.models.load_model(\n",
        "            self.config.updated_base_model_path\n",
        "        )\n",
        "\n",
        "    def train_valid_generator(self):\n",
        "\n",
        "        datagenerator_kwargs = dict(\n",
        "            rescale = 1./255,\n",
        "            validation_split=0.20\n",
        "        )\n",
        "\n",
        "        dataflow_kwargs = dict(\n",
        "            target_size=self.config.params_image_size[:-1],\n",
        "            batch_size=self.config.params_batch_size,\n",
        "            interpolation=\"bilinear\"\n",
        "        )\n",
        "\n",
        "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            **datagenerator_kwargs\n",
        "        )\n",
        "\n",
        "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
        "            directory=self.config.training_data,\n",
        "            subset=\"validation\",\n",
        "            shuffle=False,\n",
        "            **dataflow_kwargs\n",
        "        )\n",
        "\n",
        "        if self.config.params_is_augmentation:\n",
        "            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "                rotation_range=40,\n",
        "                horizontal_flip=True,\n",
        "                width_shift_range=0.2,\n",
        "                height_shift_range=0.2,\n",
        "                shear_range=0.2,\n",
        "                zoom_range=0.2,\n",
        "                **datagenerator_kwargs\n",
        "            )\n",
        "        else:\n",
        "            train_datagenerator = valid_datagenerator\n",
        "\n",
        "        self.train_generator = train_datagenerator.flow_from_directory(\n",
        "            directory=self.config.training_data,\n",
        "            subset=\"training\",\n",
        "            shuffle=True,\n",
        "            **dataflow_kwargs\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(path: Path, model: tf.keras.Model):\n",
        "        model.save(path)\n",
        "\n",
        "\n",
        "    def train(self, callback_list: list):\n",
        "        self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\n",
        "        self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size\n",
        "\n",
        "        self.model.fit(\n",
        "            self.train_generator,\n",
        "            epochs=self.config.params_epochs,\n",
        "            steps_per_epoch=self.steps_per_epoch,\n",
        "            validation_steps=self.validation_steps,\n",
        "            validation_data=self.valid_generator,\n",
        "            callbacks=callback_list\n",
        "        )\n",
        "\n",
        "        self.save_model(\n",
        "            path=self.config.trained_model_path,\n",
        "            model=self.model\n",
        "        )"
      ],
      "metadata": {
        "id": "POOuCog-t1Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    config = ConfigurationManager()\n",
        "    prepare_callbacks_config = config.get_prepare_callback_config()\n",
        "    prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
        "    callback_list = prepare_callbacks.get_tb_ckpt_callbacks()\n",
        "    \n",
        "    training_config = config.get_training_config()\n",
        "    training = Training(config=training_config)\n",
        "    training.get_base_model()\n",
        "    training.train_valid_generator()\n",
        "    training.train(\n",
        "        callback_list=callback_list\n",
        "    )\n",
        "    \n",
        "except Exception as e:\n",
        "    raise e"
      ],
      "metadata": {
        "id": "ZDRrAlBEt_sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(\"./artifacts/training/model.h5\")"
      ],
      "metadata": {
        "id": "5GogsMM4uGiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "img = Image.open(\"./artifacts/data_ingestion/PetImages/Cat/0.jpg\")\n",
        "img = img.resize((224,224))\n",
        "img_array = np.array(img)"
      ],
      "metadata": {
        "id": "JGqh2hFwwaGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array.shape"
      ],
      "metadata": {
        "id": "Cprwm_nxwcyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict(img_array)\n",
        "result"
      ],
      "metadata": {
        "id": "f54-ouI9wf5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "argmax_index = np.argmax(result, axis=1)\n",
        "if argmax_index[0] == 0:\n",
        "    print(\"predicted: cat\")\n",
        "else:\n",
        "    print(\"predicted: dog\")"
      ],
      "metadata": {
        "id": "bWbvnEEfwjAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "9TlJsTBYwmHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy('./artifacts/training/model.h5','data')"
      ],
      "metadata": {
        "id": "S-KGB1o4y4DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model(\"artifacts/training/model.h5\")"
      ],
      "metadata": {
        "id": "u0je_pLtzD_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class EvaluationConfig:\n",
        "    path_of_model: Path\n",
        "    training_data: Path\n",
        "    all_params: dict\n",
        "\n",
        "    params_image_size: list\n",
        "    params_batch_size: int"
      ],
      "metadata": {
        "id": "-AggNenWPLvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ConfigurationManager:\n",
        "    def __init__(\n",
        "        self, \n",
        "        config_filepath = CONFIG_FILE_PATH,\n",
        "        params_filepath = PARAMS_FILE_PATH):\n",
        "        self.config = read_yaml(config_filepath)\n",
        "        self.params = read_yaml(params_filepath)\n",
        "        create_directories([self.config.artifacts_root])\n",
        "\n",
        "    def get_validation_config(self) -> EvaluationConfig:\n",
        "        eval_config = EvaluationConfig(\n",
        "            path_of_model=\"artifacts/training/model.h5\",\n",
        "            training_data=\"artifacts/data_ingestion/PetImages\",\n",
        "            all_params=self.params,\n",
        "            params_image_size=self.params.IMAGE_SIZE,\n",
        "            params_batch_size=self.params.BATCH_SIZE\n",
        "        )\n",
        "        return eval_config"
      ],
      "metadata": {
        "id": "-ApIZbqAPXi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "reyu8WIFQmOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.keras\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class Evaluation:\n",
        "    def __init__(self, config: EvaluationConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def _valid_generator(self):\n",
        "\n",
        "        datagenerator_kwargs = dict(\n",
        "            rescale = 1./255,\n",
        "            validation_split=0.30\n",
        "        )\n",
        "\n",
        "        dataflow_kwargs = dict(\n",
        "            target_size=self.config.params_image_size[:-1],\n",
        "            batch_size=self.config.params_batch_size,\n",
        "            interpolation=\"bilinear\"\n",
        "        )\n",
        "\n",
        "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            **datagenerator_kwargs\n",
        "        )\n",
        "\n",
        "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
        "            directory=self.config.training_data,\n",
        "            subset=\"validation\",\n",
        "            shuffle=False,\n",
        "            **dataflow_kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(path: Path) -> tf.keras.Model:\n",
        "        return tf.keras.models.load_model(path)\n",
        "\n",
        "    def evaluation(self):\n",
        "        self.model = self.load_model(self.config.path_of_model)\n",
        "        self._valid_generator()\n",
        "        self.score = model.evaluate(self.valid_generator)\n",
        "\n",
        "    def save_score(self):\n",
        "        scores = {\"loss\": self.score[0], \"accuracy\": self.score[1]}\n",
        "        save_json(path=Path(\"scores.json\"), data=scores)\n"
      ],
      "metadata": {
        "id": "-Al-Se1mP9Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    config = ConfigurationManager()\n",
        "    val_config = config.get_validation_config()\n",
        "    evaluation = Evaluation(val_config)\n",
        "    evaluation.evaluation()\n",
        "    evaluation.save_score()\n",
        "    \n",
        "except Exception as e:\n",
        "   raise e"
      ],
      "metadata": {
        "id": "pZM5bMSDQFZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNQV4A1KQygt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}